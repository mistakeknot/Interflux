---
name: flux-gen
description: Generate project-specific review agents from detected domain profiles
argument-hint: "[optional: domain name to generate for, or 'all' for all detected domains]"
---

# Flux-Gen — Domain-Specific Agent Generator

Generate project-specific `fd-*` review agents in `.claude/agents/` based on the project's detected domain profiles. These agents complement the core flux-drive plugin agents with deeper, domain-specific review expertise.

## Step 1: Detect Project Domains

Check for cached domain detection results:

```bash
cat {PROJECT_ROOT}/.claude/flux-drive.yaml 2>/dev/null
```

Note: detect-domains.py validates cache format internally. If the cache is from an older version, it will be treated as stale and re-detected automatically.

If no cache exists, run detection:

```bash
python3 ${CLAUDE_PLUGIN_ROOT}/scripts/detect-domains.py {PROJECT_ROOT} --json
```

If `$ARGUMENTS` specifies a domain name (e.g., `game-simulation`), use only that domain. If `$ARGUMENTS` is `all` or empty, use all detected domains.

If no domains detected and no argument provided, tell the user:
> No domains detected for this project. This usually means the project is too small or doesn't match known domain signals.
>
> **Available domains:** game-simulation, web-api, ml-pipeline, cli-tool, mobile-app, embedded-systems, data-pipeline, library-sdk, tui-app, desktop-tauri, claude-code-plugin
>
> **To specify manually:** `/flux-gen game-simulation` (or any domain above)
> **To see detection signals:** Read `config/flux-drive/domains/index.yaml`
> **To skip domain agents:** Run `/flux-drive` directly — core agents work without domain specialization

## Step 2: Load Agent Specifications

For each detected domain, read the domain profile:

```
${CLAUDE_PLUGIN_ROOT}/config/flux-drive/domains/{domain-name}.md
```

Extract the `## Agent Specifications` section. Each `### fd-{name}` subsection contains:
- **Focus**: one-line description of the agent's review area (required)
- **Persona**: one-line identity and voice directive (optional — generate from Focus if absent)
- **Decision lens**: one-line prioritization heuristic (optional — generate from domain if absent)
- **Key review areas**: 5 bullet points defining what the agent checks (required)

**Generating missing fields** (when Persona or Decision lens is absent in the profile):

- **Persona fallback**: `"You are a {domain-name} {Focus} specialist — methodical, specific, and grounded in project reality."` — Replace {domain-name} with the human-readable domain name (e.g., "game simulation") and {Focus} with the Focus line.
- **Decision lens fallback**: `"Prioritize findings by real-world impact on {domain-name} projects. Flag issues that would cause failures in production before style concerns."` — Replace {domain-name} accordingly.

If the domain profile has no Agent Specifications section, skip it and note this to the user.

## Step 3: Check for Existing Agents

```bash
ls {PROJECT_ROOT}/.claude/agents/fd-*.md 2>/dev/null
```

For each agent to be generated:
- If the file already exists, **skip it** (don't overwrite user customizations)
- Present which agents exist vs which will be created

Use **AskUserQuestion** to confirm:
- Option 1: "Generate N new agents (skip M existing)" (Recommended)
- Option 2: "Regenerate all (overwrite existing)"
- Option 3: "Cancel"

## Step 4: Generate Agent Files

For each agent spec, create `.claude/agents/fd-{name}.md` using this template:

```markdown
---
generated_by: flux-gen
domain: {domain-name}
generated_at: '2026-02-09T14:30:00+00:00'  # Always UTC with explicit +00:00 offset
flux_gen_version: 3
---
# fd-{name} — {Domain} Domain Reviewer

> Generated by `/flux-gen` from the {domain-name} domain profile.
> Customize this file for your project's specific needs.

{Persona line — from profile or fallback}

## First Step (MANDATORY)

Check for project documentation:
1. `CLAUDE.md` in the project root
2. `AGENTS.md` in the project root
3. Domain-relevant docs: {domain-specific doc types — see table below}

If docs exist, operate in codebase-aware mode:
- Ground every finding in the project's actual patterns and conventions
- Reuse the project's terminology, not generic terms
- Avoid recommending changes the project has explicitly ruled out

If docs don't exist, operate in generic mode:
- Apply best practices for {domain-name} projects
- Mark assumptions explicitly so the team can correct them

## Review Approach

{For each bullet in Key review areas, create a numbered section:}

### {N}. {Short title derived from bullet}

- {Full bullet text}
- {Additional sub-criteria you can infer from the focus area — 2-3 lines expanding the bullet with specific checks}

## What NOT to Flag

- Architecture, module boundaries, or coupling concerns (fd-architecture handles this)
- Security vulnerabilities or credential handling (fd-safety handles this)
- Data consistency, race conditions, or transaction safety (fd-correctness handles this)
- Naming conventions, code style, or language idioms (fd-quality handles this)
- Rendering bottlenecks, algorithmic complexity, or memory usage (fd-performance handles this)
- User flows, UX friction, or value proposition (fd-user-product handles this)
- Only flag the above if they are deeply entangled with your domain expertise and the core agent would miss the domain-specific nuance

## Success Criteria

A good {domain-name} review:
- Ties every finding to a specific file, function, and line number — never a vague "consider X"
- Provides a concrete failure scenario for each P0/P1 finding — what breaks, under what conditions, and who is affected
- Recommends the smallest viable fix, not an architecture overhaul — one diff hunk, not a rewrite
- Distinguishes domain-specific expertise from generic code quality (defer the latter to core agents listed in "What NOT to Flag")
- Frames uncertain findings as questions: "Does this handle X?" not "This doesn't handle X"
{If the domain profile's Agent Specification includes **Success criteria hints**, append them as additional bullets here. Otherwise, use only the generic bullets above.}

## Decision Lens

{Decision lens line — from profile or fallback}

When two fixes compete for attention, choose the one with higher real-world impact on {domain-name} concerns.

## Prioritization

- P0/P1: Issues that would cause failures, data loss, or broken functionality in production
- P2: Issues that degrade quality or create maintenance burden
- P3: Improvements and polish — suggest but don't block on these
- Always tie findings to specific files, functions, and line numbers
- Frame uncertain findings as questions, not assertions
```

### Domain-specific doc types for First Step

Use these when populating the "Domain-relevant docs" line in the First Step:

| Domain | Doc types to look for |
|--------|----------------------|
| game-simulation | Game design documents (GDD), balance spreadsheets, system design docs |
| web-api | API specs (OpenAPI/Swagger), architecture decision records, runbooks |
| ml-pipeline | Model cards, experiment tracking docs, data lineage docs |
| cli-tool | Man pages, help text source, CLI design docs |
| mobile-app | Platform guidelines docs, accessibility docs, release checklists |
| embedded-systems | Hardware specs, memory maps, timing constraint docs |
| data-pipeline | Schema docs, data dictionaries, SLA definitions |
| library-sdk | API reference docs, migration guides, changelog |
| tui-app | Keybinding docs, accessibility docs, terminal compatibility notes |
| desktop-tauri | Platform integration docs, packaging configs, update channel docs |
| claude-code-plugin | Plugin manifest, skill/agent/command inventories, hook documentation |

## Step 5: Create the Agents Directory

```bash
mkdir -p {PROJECT_ROOT}/.claude/agents
```

Then write each agent file using the Write tool.

## Step 6: Report

After generation, present a summary:

```
Generated {N} project-specific agents in .claude/agents/:

Domain: {domain-name} ({confidence})
  - fd-{name1}: {Focus line}
  - fd-{name2}: {Focus line}

Domain: {domain-name2} ({confidence})
  - fd-{name3}: {Focus line}

These agents will be included in flux-drive triage as Project Agents
(+1 category bonus). Customize them by editing the .md files directly.

To use them in a review: /flux-drive <target>
To regenerate: /flux-gen (existing agents are preserved unless you choose overwrite)
```

## Notes

- Generated agents use `subagent_type: general-purpose` in flux-drive (their full content is pasted as the system prompt)
- Project Agents get +1 category bonus in triage scoring
- Multiple domains may generate overlapping agents — flux-drive deduplication handles this (prefer Project > Plugin)
- Users should customize generated agents for their specific project needs
- `flux_gen_version: 3` agents have persona, decision lens, anti-overlap clauses, and success criteria; version 1-2 agents are still valid but benefit from regeneration
